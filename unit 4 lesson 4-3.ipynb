{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sajee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\sajee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ Emma by Jane Austen 1816 ]', 'VOLUME I', 'CHAPTER I', 'Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her .']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#reading in the data, this time in the form of paragraphs\n",
    "emma=gutenberg.paras('austen-emma.txt')\n",
    "#processing\n",
    "emma_paras=[]\n",
    "for paragraph in emma:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    emma_paras.append(' '.join(para))\n",
    "\n",
    "print(emma_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1948\n",
      "Original sentence: Emma was obliged to ask what they had told her , though fearful of its producing Mr . Elton .\n",
      "Tf_idf vector: {'producing': 0.49800716119896493, 'ask': 0.37820399303630864, 'fearful': 0.46485208308634507, 'obliged': 0.3530361160312456, 'elton': 0.24540901423669018, 'told': 0.3817132197883509, 'mr': 0.18011343466017826, 'emma': 0.17279245614001376}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(emma_paras, test_size=0.4, random_state=3)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "emma_paras_tfidf=vectorizer.fit_transform(emma_paras)\n",
    "print(\"Number of features: %d\" % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(emma_paras_tfidf, test_size=0.4, random_state=3)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#applying model to test set, for challenge 0.\n",
    "X_test_tfidf_csr = X_test_tfidf.tocsr()\n",
    "\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 48.7272597198354\n",
      "Component 0:\n",
      "\" Oh !            0.999763\n",
      "\" Oh !            0.999763\n",
      "\" Oh !            0.999763\n",
      "\" Oh no , no !    0.999763\n",
      "\" Oh !            0.999763\n",
      "\" Oh !\"           0.999763\n",
      "\" Oh !            0.999763\n",
      "\" Oh !            0.999763\n",
      "\" Oh !            0.999763\n",
      "\" Oh !            0.999763\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "\" Well , Mrs . Weston ,\" said Emma triumphantly when he left them , \" what do you say now to Mr . Knightley ' s marrying Jane Fairfax ?\"                                                                                                                  0.650948\n",
      "\" I do not know what your opinion may be , Mrs . Weston ,\" said Mr . Knightley , \" of this great intimacy between Emma and Harriet Smith , but I think it a bad thing .\"                                                                                  0.583433\n",
      "\" In one respect , perhaps , Mr . Elton ' s manners are superior to Mr . Knightley ' s or Mr . Weston ' s .                                                                                                                                               0.567779\n",
      "After tea , Mr . and Mrs . Weston , and Mr . Elton sat down with Mr . Woodhouse to cards .                                                                                                                                                                0.563819\n",
      "\" I do not admire it ,\" said Mr . Knightley .                                                                                                                                                                                                             0.542263\n",
      "\" Mr . Weston will be almost as much relieved as myself ,\" said she .                                                                                                                                                                                     0.521773\n",
      "\" Isabella and Emma both write beautifully ,\" said Mr . Woodhouse ; \" and always did .                                                                                                                                                                    0.505781\n",
      "\" You get upon delicate subjects , Emma ,\" said Mrs . Weston smiling ; \" remember that I am here . Mr .                                                                                                                                                   0.500026\n",
      "Harriet , Mr . Elton , and Mr . Knightley , their own especial set , were the only persons invited to meet them ; the hours were to be early , as well as the numbers few ; Mr . Woodhouse ' s habits and inclination being consulted in every thing .    0.497878\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .                                                                                                                                                                               0.493057\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "\" Ah !      0.999936\n",
      "\" Ah !      0.999936\n",
      "\" Ah !      0.999936\n",
      "But ah !    0.999936\n",
      "\" Ah !      0.999936\n",
      "\" Ah !      0.999936\n",
      "\" Ah !      0.999936\n",
      "\" Ah !      0.999936\n",
      "\" Ah !      0.999936\n",
      "\" Ah !      0.999936\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "\" In one respect , perhaps , Mr . Elton ' s manners are superior to Mr . Knightley ' s or Mr . Weston ' s .    0.699670\n",
      "\" Mr . Knightley was there too , was he ?\"                                                                     0.697263\n",
      "Mr . Knightley grew angry .                                                                                    0.667177\n",
      "\" You are not vain , Mr . Knightley .                                                                          0.652124\n",
      "\"` Mr .                                                                                                        0.651002\n",
      "Mr . Knightley , however , shewed no triumphant happiness .                                                    0.540301\n",
      "\" Mr . Elton indeed !\"                                                                                         0.520742\n",
      "After tea , Mr . and Mrs . Weston , and Mr . Elton sat down with Mr . Woodhouse to cards .                     0.516745\n",
      "Mr . Knightley shook his head at her .                                                                         0.511175\n",
      "\" And I , Mr . Knightley , am equally stout in my confidence of its not doing them any harm .                  0.483992\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "To Miss                                                                                                                                                                           0.769523\n",
      "\" You and Miss Smith , and Miss Fairfax , will be three , and the two Miss Coxes five ,\" had been repeated many times over .                                                      0.717605\n",
      "\" Well , Miss Woodhouse !\"                                                                                                                                                        0.653172\n",
      "\" What can it be , Miss Woodhouse ? what can it be ?                                                                                                                              0.653172\n",
      "\" Charming Miss Woodhouse !                                                                                                                                                       0.626823\n",
      "\" Not five minutes to spare even for your friends Miss Fairfax and Miss Bates ?                                                                                                   0.619763\n",
      "Miss Bates and Miss Fairfax , escorted by the two gentlemen , walked into the room ; and Mrs . Elton seemed to think it as much her duty as Mrs . Weston ' s to receive them .    0.610481\n",
      "\" Mrs . Elton , I suppose , has been the person to whom Miss Fairfax owes \"                                                                                                       0.603711\n",
      "\" Perhaps Miss Fairfax has never been staying here so long before .\"                                                                                                              0.588186\n",
      "\" Come Miss Woodhouse , Miss Otway , Miss Fairfax , what are you all doing ? Come Emma , set your companions the example .                                                        0.562769\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "#fitting lsa to test set, for this particular challenge.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_test_lsa,index=X_test)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for the majority of the documents in each component are on the higher side, suggesting that the documents in each component are similar to each other and relevant to the topic.  \n",
    "\n",
    "For example, component 0 contains sentences with \"Oh!\".  Component 1 seems to be primarily focused on a couple of the main characters.  Component 2 contains sentences with the word \"Ah!\".  Component 3 contains sentences abouut the character Mr. Knightley.  Component 4 is about the characters Miss Fairfax or Miss Woodhouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 \" It is my duty , and I am sure it is my wish ,\" she continued , \" to have no reserves with you on this subject .\n",
      "1 They arrived , the carriage turned , the step was let down , and Mr . Elton , spruce , black , and smiling , was with them instantly .\n",
      "2 There was one person among his new acquaintance in Surry , not so leniently disposed .\n",
      "3 Jane very patiently assured her that she had not caught any cold .\n",
      "4 CHAPTER XVI\n",
      "5 \" It might be distressing , for the moment ,\" said she ; \" but you seem to have behaved extremely well ; and it is over  and may never  can never , as a first meeting , occur again , and therefore you need not think about it .\"\n",
      "6 \" Very well ; and if he had intended to give her one , he would have told her so .\"\n",
      "7 \" She must have some motive , more powerful than appears , for refusing this invitation ,\" was Emma ' s conclusion .\n",
      "8 \" And I have not forgotten ,\" said Emma , \" how sure you were that he might have come sooner if he would .\n",
      "9 \" Leave it to me .\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_test_lsa) * np.asmatrix(X_test_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_test).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much similarity between the sentences except for some similarity between sentences 0 and 8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
